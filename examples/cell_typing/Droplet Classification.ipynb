{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from aavomics import aavomics\n",
    "from aavomics import database\n",
    "from pepars.plotting import plotting\n",
    "import anndata\n",
    "import numpy\n",
    "import statsmodels\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "import pandas\n",
    "import scvi\n",
    "import scanpy\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get this many cells, tops. Leave to None if getting all cells\n",
    "MAX_NUM_CELLS = 200000\n",
    "\n",
    "# Which alignment to use. Set to None to use the first available\n",
    "ALIGNMENT_NAME = \"cellranger_5.0.1_gex_mm10_2020_A\"\n",
    "\n",
    "SEED = 1042\n",
    "\n",
    "TAXONOMY_NAME = \"CCN202105041\"\n",
    "\n",
    "CLUSTER_OBS_NAME = \"leiden_scVI\"\n",
    "\n",
    "TRANSFORM_TO_PLOT = \"X_tsne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_target_cells = 0\n",
    "\n",
    "for cell_set in database.CELL_SETS:\n",
    "    total_num_target_cells += cell_set.target_num_cells\n",
    "    \n",
    "downsample_factor = min(1, MAX_NUM_CELLS/total_num_target_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = []\n",
    "cell_set_names = []\n",
    "\n",
    "genes_df = None\n",
    "\n",
    "numpy.random.seed(SEED)\n",
    "        \n",
    "for cell_set_index, cell_set in enumerate(database.CELL_SETS):\n",
    "    \n",
    "    anndata_file_path = cell_set.get_anndata_file_path(alignment_name=ALIGNMENT_NAME)\n",
    "    \n",
    "    if not os.path.exists(anndata_file_path):\n",
    "        print(\"Missing %s, skipping\" % cell_set.name)\n",
    "        continue\n",
    "    \n",
    "    adata = anndata.read(anndata_file_path)\n",
    "    \n",
    "    print(cell_set.name)\n",
    "    \n",
    "    barcode_total_transcript_counts = numpy.array(adata.X.sum(axis=1)).flatten()\n",
    "    \n",
    "    cutoff = aavomics.get_background_trough(barcode_total_transcript_counts)\n",
    "    signal_mask = barcode_total_transcript_counts >= cutoff\n",
    "    \n",
    "    print(\"%s: %i droplets above threshold of %i\" % (cell_set.name, signal_mask.sum(), cutoff))\n",
    "    \n",
    "    adata = adata[signal_mask]\n",
    "    barcode_total_transcript_counts = barcode_total_transcript_counts[signal_mask]\n",
    "    \n",
    "    barcode_probabilities = barcode_total_transcript_counts / barcode_total_transcript_counts.sum()\n",
    "    barcode_indices = list(range(len(barcode_probabilities)))\n",
    "    \n",
    "    num_cells = int(numpy.round(cell_set.target_num_cells * downsample_factor))\n",
    "    \n",
    "    weighted_random_barcode_indices = numpy.random.choice(\n",
    "        barcode_indices,\n",
    "        size=num_cells,\n",
    "        p=barcode_probabilities,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    adata = adata[weighted_random_barcode_indices]\n",
    "    \n",
    "    adatas.append(adata)\n",
    "    cell_set_names.append(cell_set.name)\n",
    "    \n",
    "merged_adata = adatas[0].concatenate(adatas[1:], batch_key=\"Cell Set\", batch_categories=numpy.array(cell_set_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_maxes = merged_adata.X.max(axis=0)\n",
    "gene_maxes = numpy.array(gene_maxes.todense()).flatten()\n",
    "gene_mask = (gene_maxes > 0)\n",
    "merged_adata = merged_adata[:, gene_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_categorical_type = pandas.CategoricalDtype(categories=[\"Debris\", \"Multiplets\", \"Neurons\", \"Non-Neurons\", \"\"])\n",
    "\n",
    "merged_adata.obs[TAXONOMY_NAME] = pandas.Series(dtype=cell_type_categorical_type)\n",
    "merged_adata.obs[TAXONOMY_NAME].loc[:] = \"Debris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.data.setup_anndata(merged_adata, batch_key=\"Cell Set\", labels_key=TAXONOMY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = scvi.model.SCVI(\n",
    "    merged_adata,\n",
    "    n_latent=20,\n",
    "    n_layers=2,\n",
    "    n_hidden=256\n",
    ")\n",
    "\n",
    "vae.train(\n",
    "    frequency=1,\n",
    "    n_epochs_kl_warmup=None,\n",
    "    n_iter_kl_warmup=128*5000/400, # Based on documentation at https://www.scvi-tools.org/en/stable/api/reference/scvi.core.trainers.UnsupervisedTrainer.html\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adata.obsm[\"X_scVI\"] = vae.get_latent_representation(merged_adata)\n",
    "scanpy.pp.neighbors(merged_adata, use_rep=\"X_scVI\", random_state=SEED)\n",
    "scanpy.tl.leiden(merged_adata, key_added=CLUSTER_OBS_NAME, random_state=SEED, resolution=2) # Resolution 2 to distinguish between doublet clusters\n",
    "scanpy.tl.tsne(merged_adata, use_rep=\"X_scVI\", n_jobs=16, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_gene_expression = vae.get_normalized_expression(merged_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], merged_adata.obs[\"Cell Set\"], filename=os.path.join(\"out\", \"samples.html\"))\n",
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], merged_adata.obs[CLUSTER_OBS_NAME], filename=os.path.join(\"out\", \"clusters.html\"))\n",
    "total_transcript_counts = numpy.array(merged_adata.X.sum(axis=1)).flatten()\n",
    "aavomics.plot_gene_expression(merged_adata.obsm[TRANSFORM_TO_PLOT], total_transcript_counts, filename=os.path.join(\"out\", \"total_transcript_counts.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_gene_non_zeros_df = pandas.DataFrame(index=sorted(merged_adata.obs[CLUSTER_OBS_NAME].unique().astype(numpy.uint16)), columns=merged_adata.var.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scvi import _CONSTANTS\n",
    "from scvi.core.distributions import NegativeBinomial, ZeroInflatedNegativeBinomial\n",
    "from typing import Dict, Iterable, Optional, Sequence, Union\n",
    "from anndata import AnnData\n",
    "\n",
    "from scvi.model._utils import (\n",
    "    _get_batch_code_from_category,\n",
    "    _get_var_names_from_setup_anndata,\n",
    "    scrna_raw_counts_properties,\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def posterior_predictive_sample(\n",
    "    self,\n",
    "    adata: Optional[AnnData] = None,\n",
    "    indices: Optional[Sequence[int]] = None,\n",
    "    n_samples: int = 1,\n",
    "    gene_list: Optional[Sequence[str]] = None,\n",
    "    batch_size: Optional[int] = None,\n",
    "    transform_batch: Optional[int] = None\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Generate observation samples from the posterior predictive distribution.\n",
    "\n",
    "    The posterior predictive distribution is written as :math:`p(\\hat{x} \\mid x)`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "        AnnData object used to initialize the model.\n",
    "    indices\n",
    "        Indices of cells in adata to use. If `None`, all cells are used.\n",
    "    n_samples\n",
    "        Number of samples for each cell.\n",
    "    gene_list\n",
    "        Names of genes of interest.\n",
    "    batch_size\n",
    "        Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_new : :py:class:`torch.Tensor`\n",
    "        tensor with shape (n_cells, n_genes, n_samples)\n",
    "    \"\"\"\n",
    "    if self.model.gene_likelihood not in [\"zinb\", \"nb\", \"poisson\"]:\n",
    "        raise ValueError(\"Invalid gene_likelihood.\")\n",
    "\n",
    "    adata = self._validate_anndata(adata)\n",
    "    scdl = self._make_scvi_dl(adata=adata, indices=indices, batch_size=batch_size)\n",
    "\n",
    "    if indices is None:\n",
    "        indices = np.arange(adata.n_obs)\n",
    "\n",
    "    if gene_list is None:\n",
    "        gene_mask = slice(None)\n",
    "    else:\n",
    "        all_genes = _get_var_names_from_setup_anndata(adata)\n",
    "        gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "    x_new = []\n",
    "    for tensors in scdl:\n",
    "        x = tensors[_CONSTANTS.X_KEY]\n",
    "        batch_idx = tensors[_CONSTANTS.BATCH_KEY]\n",
    "        labels = tensors[_CONSTANTS.LABELS_KEY]\n",
    "        outputs = self.model.inference(\n",
    "            x, batch_index=batch_idx, y=labels, n_samples=n_samples, transform_batch=transform_batch\n",
    "        )\n",
    "        px_r = outputs[\"px_r\"]\n",
    "        px_rate = outputs[\"px_rate\"]\n",
    "        px_dropout = outputs[\"px_dropout\"]\n",
    "\n",
    "        if self.model.gene_likelihood == \"poisson\":\n",
    "            l_train = px_rate\n",
    "            l_train = torch.clamp(l_train, max=1e8)\n",
    "            dist = torch.distributions.Poisson(\n",
    "                l_train\n",
    "            )  # Shape : (n_samples, n_cells_batch, n_genes)\n",
    "        elif self.model.gene_likelihood == \"nb\":\n",
    "            dist = NegativeBinomial(mu=px_rate, theta=px_r)\n",
    "        elif self.model.gene_likelihood == \"zinb\":\n",
    "            dist = ZeroInflatedNegativeBinomial(\n",
    "                mu=px_rate, theta=px_r, zi_logits=px_dropout\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} reconstruction error not handled right now\".format(\n",
    "                    self.model.gene_likelihood\n",
    "                )\n",
    "            )\n",
    "        if n_samples > 1:\n",
    "            exprs = dist.sample().permute(\n",
    "                [1, 2, 0]\n",
    "            )  # Shape : (n_cells_batch, n_genes, n_samples)\n",
    "        else:\n",
    "            exprs = dist.sample()\n",
    "\n",
    "        if gene_list is not None:\n",
    "            exprs = exprs[:, gene_mask, ...]\n",
    "\n",
    "        x_new.append(exprs.cpu())\n",
    "    x_new = torch.cat(x_new)  # Shape (n_cells, n_genes, n_samples)\n",
    "\n",
    "    return x_new.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = sorted(merged_adata.obs[CLUSTER_OBS_NAME].unique(), key=lambda x: int(x))\n",
    "\n",
    "NUM_SAMPLES = 5000\n",
    "\n",
    "batch_cluster_gene_non_zeros_dfs = {}\n",
    "\n",
    "for batch_id in merged_adata.obs[\"_scvi_batch\"].unique():\n",
    "    \n",
    "    cluster_gene_non_zeros_df = pandas.DataFrame(index=sorted(merged_adata.obs[CLUSTER_OBS_NAME].unique().astype(numpy.uint16)), columns=merged_adata.var.index)\n",
    "\n",
    "    for cluster in clusters:\n",
    "\n",
    "        print(\"Batch %i cluster %s\" % (batch_id, cluster))\n",
    "\n",
    "        adata_copy = merged_adata[merged_adata.obs[CLUSTER_OBS_NAME] == cluster].copy()\n",
    "\n",
    "        random_sample = adata_copy[numpy.random.choice(list(range(0, adata_copy.shape[0])), replace=True, size=NUM_SAMPLES)].copy()\n",
    "\n",
    "        samples = posterior_predictive_sample(vae, random_sample, n_samples=1, transform_batch=batch_id)\n",
    "\n",
    "        non_zero_counts = (samples != 0).sum(axis=0)\n",
    "\n",
    "        p_non_zeros = numpy.array(non_zero_counts/NUM_SAMPLES)\n",
    "\n",
    "        cluster_gene_non_zeros_df.loc[int(cluster), :] = p_non_zeros\n",
    "        \n",
    "    batch_cluster_gene_non_zeros_dfs[batch_id] = cluster_gene_non_zeros_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_ratios = numpy.array(merged_adata[:, merged_adata.var[\"Gene Name\"].str.startswith(\"mt-\")].X.sum(axis=1)/merged_adata.X.sum(axis=1)).flatten()\n",
    "aavomics.plot_gene_expression(merged_adata.obsm[TRANSFORM_TO_PLOT], mt_ratios, filename=os.path.join(\"out\", \"mt_ratios.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER_GENE_CELL_TYPES = {\n",
    "\n",
    "    \"Sox9\": \"Astrocytes\",\n",
    "\n",
    "    \"Cldn5\": \"Vascular Cells\", # Endothelial Cells\n",
    "    \"Pdgfrb\": \"Vascular Cells\", # Pericytes\n",
    "    \"Hba-a1\": \"Vascular Cells\", # Red Blood Cells\n",
    "\n",
    "    \"Rbfox3\": \"Neurons\", # Neurons\n",
    "\n",
    "    \"Cx3cr1\": \"Immune Cells\", # Microglia\n",
    "    \"Mrc1\": \"Immune Cells\", #\"Perivascular Macrophages\"\n",
    "\n",
    "    \"Olig1\": \"Oligodendrocytes\", # Oligodendrocytes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = []\n",
    "\n",
    "clusters = merged_adata.obs[CLUSTER_OBS_NAME].unique().astype(numpy.uint16)\n",
    "batches = merged_adata.obs[\"_scvi_batch\"].unique()\n",
    "\n",
    "marker_gene_cluster_batch_counts = {}\n",
    "batch_ratio_thresholds = {}\n",
    "\n",
    "for marker_gene in MARKER_GENE_CELL_TYPES:\n",
    "    \n",
    "    ensembl_id = merged_adata.var.loc[merged_adata.var['Gene Name']==marker_gene].index[0]\n",
    "    \n",
    "    cluster_batch_counts = {cluster: 0 for cluster in clusters}\n",
    "\n",
    "    for batch_id in batches:\n",
    "\n",
    "        cluster_gene_non_zeros_df = pandas.DataFrame(index=sorted(clusters), columns=merged_adata.var.index)\n",
    "\n",
    "        for cluster in cluster_gene_non_zeros_df.index.values:\n",
    "            \n",
    "            cluster_gene_non_zeros_df.loc[int(cluster), :] = batch_cluster_gene_non_zeros_dfs[batch_id].loc[int(cluster)]\n",
    "\n",
    "        values = cluster_gene_non_zeros_df[ensembl_id].astype(numpy.float32).values\n",
    "\n",
    "        nan_filter = ~numpy.isnan(values)\n",
    "        values = values[nan_filter].reshape((-1, 1))\n",
    "        \n",
    "        threshold = threshold_otsu(values)\n",
    "\n",
    "        clusters_above_threshold = cluster_gene_non_zeros_df[nan_filter].index[values.flatten() > threshold]\n",
    "        \n",
    "        for cluster in clusters_above_threshold:\n",
    "            cluster_batch_counts[cluster] += 1\n",
    "            \n",
    "        marker_gene_cluster_batch_counts[marker_gene] = cluster_batch_counts\n",
    "\n",
    "    all_counts.extend(cluster_batch_counts.values())\n",
    "\n",
    "    batch_ratio_threshold = threshold_otsu(numpy.array(list(cluster_batch_counts.values()))/len(batches))\n",
    "    batch_ratio_thresholds[marker_gene] = batch_ratio_threshold\n",
    "\n",
    "batch_ratio_threshold = threshold_otsu(numpy.array(all_counts)/len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = merged_adata.obs[CLUSTER_OBS_NAME].unique().astype(numpy.uint16)\n",
    "\n",
    "marker_gene_clusters = {}\n",
    "\n",
    "for marker_gene in MARKER_GENE_CELL_TYPES:\n",
    "    \n",
    "    marker_gene_clusters[marker_gene] = []\n",
    "    \n",
    "    for cluster, batch_count in marker_gene_cluster_batch_counts[marker_gene].items():\n",
    "        \n",
    "        if batch_count/len(batches) > batch_ratio_thresholds[marker_gene]:\n",
    "            marker_gene_clusters[marker_gene].append(cluster)\n",
    "    \n",
    "    print(\"%s clusters above threshold: \" % marker_gene, marker_gene_clusters[marker_gene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_clusters = {cell_type: set() for cell_type in cell_type_categorical_type.categories}\n",
    "\n",
    "for cluster in cluster_gene_non_zeros_df.index.values:\n",
    "    \n",
    "    cluster_cell_types = set()\n",
    "    \n",
    "    cell_type_counts = {}\n",
    "    max_cell_type_counts = {}\n",
    "    \n",
    "    for marker_gene in MARKER_GENE_CELL_TYPES:\n",
    "        \n",
    "        cell_type = MARKER_GENE_CELL_TYPES[marker_gene]\n",
    "        \n",
    "        if cell_type in max_cell_type_counts:\n",
    "            max_cell_type_counts[cell_type] += 1\n",
    "        else:\n",
    "            max_cell_type_counts[cell_type] = 1\n",
    "        \n",
    "        if cluster in marker_gene_clusters[marker_gene]:\n",
    "            \n",
    "            if cell_type in cell_type_counts:\n",
    "                cell_type_counts[cell_type] += 1\n",
    "            else:\n",
    "                cell_type_counts[cell_type] = 1\n",
    "    \n",
    "    if \"Neurons\" in cell_type_counts:\n",
    "        if len(cell_type_counts) == 1:\n",
    "            cell_type_clusters[\"Neurons\"].add(cluster)\n",
    "        else:\n",
    "            cell_type_clusters[\"Debris\"].add(cluster)\n",
    "    elif len(cell_type_counts) == 0:\n",
    "        cell_type_clusters[\"Debris\"].add(cluster)\n",
    "    else:\n",
    "        cell_type_clusters[\"Non-Neurons\"].add(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_clusters = numpy.empty((merged_adata.shape[0]), dtype=numpy.object)\n",
    "automatic_clusters[:] = \"Debris\"\n",
    "\n",
    "for cell_type, clusters in cell_type_clusters.items():\n",
    "    for cluster in clusters:\n",
    "        automatic_clusters[merged_adata.obs[CLUSTER_OBS_NAME] == str(cluster)] = cell_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adata.obs[TAXONOMY_NAME] = merged_adata.obs[TAXONOMY_NAME].astype(cell_type_categorical_type)\n",
    "merged_adata.obs[TAXONOMY_NAME].loc[:] = automatic_clusters\n",
    "merged_adata.obs[TAXONOMY_NAME] = merged_adata.obs[TAXONOMY_NAME].astype(cell_type_categorical_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_of_interest = MARKER_GENE_CELL_TYPES\n",
    "\n",
    "for gene in list(genes_of_interest):\n",
    "\n",
    "    ensembl_id = merged_adata.var.loc[merged_adata.var['Gene Name']==gene].index[0]\n",
    "    raw_gene_counts = numpy.array(normalized_gene_expression.loc[:, ensembl_id].values).reshape((-1,))\n",
    "\n",
    "    aavomics.plot_gene_expression(merged_adata.obsm[TRANSFORM_TO_PLOT], numpy.log2(raw_gene_counts), filename=os.path.join(\"out\", \"gene_expression_%s_normalized.html\" % gene))\n",
    "\n",
    "    raw_gene_counts = numpy.array(merged_adata[:, ensembl_id].X.todense()).reshape((-1,))\n",
    "\n",
    "    aavomics.plot_gene_expression(merged_adata.obsm[TRANSFORM_TO_PLOT], raw_gene_counts, filename=os.path.join(\"out\", \"gene_expression_%s_raw.html\" % gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], merged_adata.obs[TAXONOMY_NAME], filename=os.path.join(\"out\", \"%s.html\" % TAXONOMY_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrublet\n",
    "\n",
    "merged_adata.obs[\"doublet\"] = numpy.array(merged_adata.obs[\"Cell Set\"] == None).astype(numpy.bool)\n",
    "merged_adata.obs[\"p_doublet\"] = numpy.zeros((merged_adata.shape[0]))\n",
    "\n",
    "for cell_set in merged_adata.obs[\"Cell Set\"].unique():\n",
    "    \n",
    "    print(cell_set)\n",
    "    \n",
    "    mask = merged_adata.obs[\"Cell Set\"] == cell_set\n",
    "    \n",
    "    cell_set_counts = merged_adata[mask].X\n",
    "    \n",
    "    scrub = scrublet.Scrublet(cell_set_counts)\n",
    "    \n",
    "    doublet_scores, predicted_doublets = scrub.scrub_doublets()\n",
    "    \n",
    "    if predicted_doublets is not None:\n",
    "        merged_adata.obs.loc[mask, \"doublet\"] = predicted_doublets\n",
    "        merged_adata.obs.loc[mask, \"p_doublet\"] = doublet_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], merged_adata.obs[\"doublet\"], filename=os.path.join(\"out\", \"doublet.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = []\n",
    "\n",
    "for cluster in merged_adata.obs[CLUSTER_OBS_NAME].unique():\n",
    "    \n",
    "    cluster_mask = merged_adata.obs[CLUSTER_OBS_NAME] == cluster\n",
    "    cluster_multiplet_mask = cluster_mask & merged_adata.obs[\"doublet\"]\n",
    "    \n",
    "    percent = cluster_multiplet_mask.sum()/cluster_mask.sum()*100\n",
    "    percents.append(percent)\n",
    "\n",
    "percent_threshold = threshold_otsu(numpy.array(percents))\n",
    "percent_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adata.obs[TAXONOMY_NAME].loc[:] = automatic_clusters\n",
    "merged_adata.obs[TAXONOMY_NAME] = merged_adata.obs[TAXONOMY_NAME].astype(cell_type_categorical_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adata.obs.loc[(merged_adata.obs[\"doublet\"]) & (merged_adata.obs[TAXONOMY_NAME] != \"Debris\"), TAXONOMY_NAME] = \"Multiplets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in merged_adata.obs[CLUSTER_OBS_NAME].unique():\n",
    "    \n",
    "    cluster_mask = merged_adata.obs[CLUSTER_OBS_NAME] == cluster\n",
    "    cluster_multiplet_mask = cluster_mask & merged_adata.obs[\"doublet\"]\n",
    "    \n",
    "    percent = cluster_multiplet_mask.sum()/cluster_mask.sum()*100\n",
    "    \n",
    "    if percent > percent_threshold:\n",
    "        merged_adata.obs.loc[cluster_mask, TAXONOMY_NAME] = \"Multiplets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], merged_adata.obs[TAXONOMY_NAME], filename=os.path.join(\"out\", \"%s.html\" % TAXONOMY_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.data.setup_anndata(merged_adata, batch_key=\"Cell Set\", labels_key=TAXONOMY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanvi = scvi.model.SCANVI(\n",
    "    merged_adata,\n",
    "    unlabeled_category=\"\",\n",
    "    pretrained_model=vae,\n",
    "    n_latent=20,\n",
    "    n_layers=2,\n",
    "    n_hidden=256\n",
    ")\n",
    "\n",
    "results = scanvi.train(\n",
    "    unsupervised_trainer_kwargs={\n",
    "        \"seed\": SEED + 1\n",
    "    },\n",
    "    semisupervised_trainer_kwargs={\n",
    "        \"seed\": SEED + 2,\n",
    "        \"n_iter_kl_warmup\": 128*5000/400,\n",
    "        \"n_epochs_kl_warmup\": None\n",
    "    },\n",
    "    balanced_sampling=True,\n",
    "    frequency=1,\n",
    "    n_epochs_kl_warmup=None,\n",
    "    n_iter_kl_warmup=128*5000/400, # Based on documentation at https://www.scvi-tools.org/en/stable/api/reference/scvi.core.trainers.UnsupervisedTrainer.html\n",
    ")\n",
    "\n",
    "scanvi.save(\"droplet_classifier\")\n",
    "\n",
    "merged_adata.write_h5ad(os.path.join(database.DATA_PATH, \"aavomics_mouse_cortex_2021_droplet_training_data.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = scanvi.trainer.train_test_validation()[0].indices\n",
    "test_indices = scanvi.trainer.train_test_validation()[1].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = scanvi.predict(merged_adata)\n",
    "prediction_scores = scanvi.predict(merged_adata, soft=True)\n",
    "prediction_scores_max = prediction_scores.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (100*(predicted_labels == merged_adata.obs[TAXONOMY_NAME]).sum()/merged_adata.shape[0]))\n",
    "print(\"Train Accuracy: %.2f%%\" % (100*(predicted_labels[train_indices] == merged_adata[train_indices].obs[TAXONOMY_NAME]).sum()/train_indices.shape[0]))\n",
    "print(\"Test Accuracy: %.2f%%\" % (100*(predicted_labels[test_indices] == merged_adata[test_indices].obs[TAXONOMY_NAME]).sum()/test_indices.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aavomics.plot_clusters(merged_adata.obsm[TRANSFORM_TO_PLOT], predicted_labels, filename=os.path.join(\"out\", \"%s_predicted.html\" % TAXONOMY_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = [TAXONOMY_NAME, \"p_%s\" % TAXONOMY_NAME, \"doublet\", \"p_doublet\"]\n",
    "\n",
    "for cell_set_index, cell_set in enumerate(database.CELL_SETS):\n",
    "    \n",
    "    anndata_file_path = cell_set.get_anndata_file_path(alignment_name=ALIGNMENT_NAME)\n",
    "    \n",
    "    if not os.path.exists(anndata_file_path):\n",
    "        print(\"Missing %s, skipping\" % cell_set.name)\n",
    "        continue\n",
    "        \n",
    "    print(cell_set.name)\n",
    "    \n",
    "    adata = anndata.read(anndata_file_path)\n",
    "    \n",
    "    for column in COLUMNS_TO_DROP:\n",
    "    \n",
    "        if column in adata.obs.columns:\n",
    "            adata.obs.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    barcode_total_transcript_counts = numpy.array(adata.X.sum(axis=1)).flatten()\n",
    "    cutoff = aavomics.get_background_trough(barcode_total_transcript_counts)\n",
    "    signal_mask = barcode_total_transcript_counts >= cutoff\n",
    "    \n",
    "    filtered_adata = adata[signal_mask].copy()\n",
    "    filtered_adata = filtered_adata[:, merged_adata.var.index.values].copy()\n",
    "    filtered_adata.X = filtered_adata.X.tocsr()\n",
    "    filtered_adata.obs[\"Cell Set\"] = pandas.Series(dtype=merged_adata.obs[\"Cell Set\"].dtype)\n",
    "    filtered_adata.obs[\"Cell Set\"].loc[:] = cell_set.name\n",
    "    \n",
    "    filtered_adata.obs[TAXONOMY_NAME] = pandas.Series(dtype=merged_adata.obs[TAXONOMY_NAME].dtype)\n",
    "    filtered_adata.obs[TAXONOMY_NAME].loc[:] = \"Debris\"\n",
    "    \n",
    "    scvi.data.setup_anndata(filtered_adata, batch_key=\"Cell Set\", labels_key=TAXONOMY_NAME)\n",
    "    \n",
    "    test_predicted_labels = scanvi.predict(filtered_adata)\n",
    "    test_prediction_scores = scanvi.predict(filtered_adata, soft=True)\n",
    "    test_prediction_scores_max = test_prediction_scores.max(axis=1)\n",
    "    filtered_adata.obs[TAXONOMY_NAME] = test_predicted_labels\n",
    "    \n",
    "    p_values_corrected = statsmodels.stats.multitest.multipletests(1-test_prediction_scores_max, method=\"fdr_bh\", alpha=0.05)\n",
    "    p_values_corrected_mask = p_values_corrected[0]\n",
    "    \n",
    "    not_debris_adata = filtered_adata[p_values_corrected_mask & (test_predicted_labels != \"Debris\") & (test_predicted_labels != \"Multiplets\")].copy()\n",
    "    \n",
    "    cell_set_counts = not_debris_adata.X\n",
    "    \n",
    "    scrub = scrublet.Scrublet(cell_set_counts)\n",
    "    \n",
    "    doublet_scores, predicted_doublets = scrub.scrub_doublets()\n",
    "    \n",
    "    if predicted_doublets is None:\n",
    "        predicted_doublets = numpy.zeros((not_debris_adata.shape[0],)).astype(numpy.bool)\n",
    "        \n",
    "    adata.obs.loc[not_debris_adata.obs.index, \"doublet\"] = predicted_doublets\n",
    "    adata.obs.loc[not_debris_adata.obs.index, \"p_doublet\"] = doublet_scores\n",
    "    \n",
    "    adata.obs[\"Cell Called\"] = pandas.Series(dtype=numpy.bool)\n",
    "    adata.obs[\"Cell Called\"].loc[:] = False\n",
    "    \n",
    "    adata.obs.loc[not_debris_adata[~predicted_doublets].obs.index, \"Cell Called\"] = True\n",
    "\n",
    "    num_cells = (adata.obs[\"Cell Called\"] == True).sum()\n",
    "    \n",
    "    adata.obs.loc[filtered_adata.obs.index, TAXONOMY_NAME] = filtered_adata.obs[TAXONOMY_NAME]\n",
    "    adata.obs.loc[filtered_adata.obs.index, \"p_%s\" % TAXONOMY_NAME] = test_prediction_scores_max\n",
    "    \n",
    "    print(cell_set.name, num_cells)\n",
    "    adata.write_h5ad(anndata_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
